{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install d2l==1.0.0-alpha1.post0\n",
        "import json\n",
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import f1_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "HtYkEyeU9mrG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "617fe460-b35e-4f6f-e8c1-33b728cb62b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting d2l==1.0.0-alpha1.post0\n",
            "  Downloading d2l-1.0.0a1.post0-py3-none-any.whl (93 kB)\n",
            "\u001b[K     |████████████████████████████████| 93 kB 994 kB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from d2l==1.0.0-alpha1.post0) (3.2.2)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from d2l==1.0.0-alpha1.post0) (0.25.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from d2l==1.0.0-alpha1.post0) (1.21.6)\n",
            "Collecting jupyter\n",
            "  Downloading jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)\n",
            "Collecting matplotlib-inline\n",
            "  Downloading matplotlib_inline-0.1.6-py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from d2l==1.0.0-alpha1.post0) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from d2l==1.0.0-alpha1.post0) (1.3.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym->d2l==1.0.0-alpha1.post0) (1.5.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym->d2l==1.0.0-alpha1.post0) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym->d2l==1.0.0-alpha1.post0) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym->d2l==1.0.0-alpha1.post0) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym->d2l==1.0.0-alpha1.post0) (3.8.1)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l==1.0.0-alpha1.post0) (5.3.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l==1.0.0-alpha1.post0) (5.6.1)\n",
            "Collecting qtconsole\n",
            "  Downloading qtconsole-5.3.2-py3-none-any.whl (120 kB)\n",
            "\u001b[K     |████████████████████████████████| 120 kB 17.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l==1.0.0-alpha1.post0) (6.1.0)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l==1.0.0-alpha1.post0) (5.3.4)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l==1.0.0-alpha1.post0) (7.7.1)\n",
            "Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (7.9.0)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (6.1.12)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (5.1.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (0.2.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (4.4.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (2.6.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (0.7.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (57.4.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (2.0.10)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 63.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (0.8.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (1.15.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->d2l==1.0.0-alpha1.post0) (3.0.3)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->d2l==1.0.0-alpha1.post0) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->d2l==1.0.0-alpha1.post0) (3.6.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l==1.0.0-alpha1.post0) (1.8.0)\n",
            "Requirement already satisfied: jupyter-core>=4.4.0 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l==1.0.0-alpha1.post0) (4.11.1)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l==1.0.0-alpha1.post0) (5.4.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l==1.0.0-alpha1.post0) (0.13.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l==1.0.0-alpha1.post0) (2.11.3)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (2.8.2)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (23.2.1)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter->d2l==1.0.0-alpha1.post0) (0.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook->jupyter->d2l==1.0.0-alpha1.post0) (2.0.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->d2l==1.0.0-alpha1.post0) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->d2l==1.0.0-alpha1.post0) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->d2l==1.0.0-alpha1.post0) (0.11.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==1.0.0-alpha1.post0) (0.6.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==1.0.0-alpha1.post0) (0.8.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==1.0.0-alpha1.post0) (1.5.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==1.0.0-alpha1.post0) (5.0.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==1.0.0-alpha1.post0) (0.4)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==1.0.0-alpha1.post0) (0.7.1)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook->jupyter->d2l==1.0.0-alpha1.post0) (2.16.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook->jupyter->d2l==1.0.0-alpha1.post0) (4.3.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook->jupyter->d2l==1.0.0-alpha1.post0) (22.1.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook->jupyter->d2l==1.0.0-alpha1.post0) (5.9.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook->jupyter->d2l==1.0.0-alpha1.post0) (0.18.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter->d2l==1.0.0-alpha1.post0) (0.5.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->d2l==1.0.0-alpha1.post0) (2022.2.1)\n",
            "Collecting qtpy>=2.0.1\n",
            "  Downloading QtPy-2.2.0-py3-none-any.whl (82 kB)\n",
            "\u001b[K     |████████████████████████████████| 82 kB 636 kB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from qtpy>=2.0.1->qtconsole->jupyter->d2l==1.0.0-alpha1.post0) (21.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->d2l==1.0.0-alpha1.post0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->d2l==1.0.0-alpha1.post0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->d2l==1.0.0-alpha1.post0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->d2l==1.0.0-alpha1.post0) (2022.6.15)\n",
            "Installing collected packages: jedi, qtpy, qtconsole, matplotlib-inline, jupyter, d2l\n",
            "Successfully installed d2l-1.0.0a1.post0 jedi-0.18.1 jupyter-1.0.0 matplotlib-inline-0.1.6 qtconsole-5.3.2 qtpy-2.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9fKXzj8K9zx"
      },
      "outputs": [],
      "source": [
        "DATAPATH = \"drive/MyDrive/SML2\"\n",
        "from collections import Counter\n",
        "def dict_from_json(filename):\n",
        "    f = open(filename)\n",
        "    data = json.load(f)\n",
        "    f.close()\n",
        "    return data\n",
        "\n",
        "train_dict = dict_from_json(\"drive/MyDrive/SML2/alltrain.json\")\n",
        "valid_dict = dict_from_json(\"drive/MyDrive/SML2/test.json\")\n",
        "tests = [train_dict[i]['abstract'] for i in range(len(train_dict))] + [valid_dict[i]['abstract'] for i in range(len(valid_dict))]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class myWikiTextDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Defined in :numref:`subsec_prepare_mlm_data`\"\"\"\n",
        "    def __init__(self, paragraphs, max_len):\n",
        "        # Input `paragraphs[i]` is a list of sentence strings representing a\n",
        "        # paragraph; while output `paragraphs[i]` is a list of sentences\n",
        "        # representing a paragraph, where each sentence is a list of tokens\n",
        "        paragraphs = [paragraph for paragraph in paragraphs]\n",
        "        sentences = [sentence for paragraph in paragraphs\n",
        "                     for sentence in paragraph]\n",
        "        self.vocab = MYVocab(sentences, min_freq=5, reserved_tokens=[\n",
        "            '<pad>', '<mask>', '<cls>', '<sep>'])\n",
        "        # Get data for the next sentence prediction task\n",
        "        examples = []\n",
        "        for paragraph in paragraphs:\n",
        "            examples.extend(d2l._get_nsp_data_from_paragraph(\n",
        "                paragraph, paragraphs, self.vocab, max_len))\n",
        "        # Get data for the masked language model task\n",
        "        examples = [(d2l._get_mlm_data_from_tokens(tokens, self.vocab)\n",
        "                      + (segments, is_next))\n",
        "                     for tokens, segments, is_next in examples]\n",
        "        self.examples = examples\n",
        "        # Pad inputs\n",
        "        (self.all_token_ids, self.all_segments, self.valid_lens,\n",
        "         self.all_pred_positions, self.all_mlm_weights,\n",
        "         self.all_mlm_labels, self.nsp_labels) = d2l._pad_bert_inputs(\n",
        "            examples, max_len, self.vocab)\n",
        "         \n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (self.all_token_ids[idx], self.all_segments[idx],\n",
        "                self.valid_lens[idx], self.all_pred_positions[idx],\n",
        "                self.all_mlm_weights[idx], self.all_mlm_labels[idx],\n",
        "                self.nsp_labels[idx])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.all_token_ids)\n",
        "\n",
        "class MYVocab:\n",
        "    \"\"\"Vocabulary for text.\"\"\"\n",
        "    def __init__(self, tokens=[], min_freq=0, reserved_tokens=[]):\n",
        "        \"\"\"Defined in :numref:`sec_text-sequence`\"\"\"\n",
        "        # Flatten a 2D list if needed\n",
        "        if tokens and isinstance(tokens[0], list):\n",
        "            tokens = [token for line in tokens for token in line]\n",
        "        # Count token frequencies\n",
        "        counter = Counter(tokens)\n",
        "        self.token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
        "                                  reverse=True)\n",
        "        # The list of unique tokens\n",
        "        comp1 = reserved_tokens\n",
        "        comp2 = [token for token, freq in self.token_freqs if freq >= min_freq]\n",
        "        self.idx_to_token = list(set(['<unk>'] + comp1 + comp2))\n",
        "        self.token_to_idx = {token: idx\n",
        "                             for idx, token in enumerate(self.idx_to_token)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx_to_token)\n",
        "\n",
        "    def __getitem__(self, tokens):\n",
        "        if not isinstance(tokens, (list, tuple)):\n",
        "            return self.token_to_idx.get(tokens, self.unk)\n",
        "        return [self.__getitem__(token) for token in tokens]\n",
        "\n",
        "    def to_tokens(self, indices):\n",
        "        if hasattr(indices, '__len__') and len(indices) > 1:\n",
        "            return [self.idx_to_token[int(index)] for index in indices]\n",
        "        return self.idx_to_token[indices]\n",
        "\n",
        "    @property\n",
        "    def unk(self):  # Index for the unknown token\n",
        "        return self.token_to_idx['<unk>']"
      ],
      "metadata": {
        "id": "mQA3pCyDMcxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def paragraph_builder(abstract):\n",
        "    size = len(abstract)\n",
        "    if abstract[-1] != 12:\n",
        "      abstract.append(12)\n",
        "    idx_list = [idx + 1 for idx, val in\n",
        "            enumerate(abstract) if val == 12]\n",
        "    \n",
        "    res = [abstract[i: j] for i, j in\n",
        "            zip([0] + idx_list, idx_list + \n",
        "            ([size] if idx_list[-1] != size else []))]\n",
        "    return res\n",
        "\n",
        "def load_Mydataset(batch_size, max_len, asbtracts):\n",
        "    num_workers = 0\n",
        "    paragraphs = [paragraph_builder(paragraph) for paragraph in asbtracts]\n",
        "    train_set = myWikiTextDataset(paragraphs, max_len)\n",
        "    train_iter = torch.utils.data.DataLoader(train_set, batch_size,shuffle=True, num_workers=num_workers)\n",
        "    return train_iter, train_set.vocab"
      ],
      "metadata": {
        "id": "AWWvDBphMgKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_iter, vocab = load_Mydataset(32, 128, tests)"
      ],
      "metadata": {
        "id": "lYcrZ89gMug_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUMS_OF_HEAD = 8\n",
        "NUMS_OF_BLKS = 8\n",
        "DROPOUT = 0.2\n",
        "HIDDEN = 128\n",
        "FFN_num_hiddens = 256\n",
        "\n",
        "net = d2l.BERTModel(len(vocab), num_hiddens=HIDDEN,\n",
        "                    ffn_num_hiddens=FFN_num_hiddens, num_heads=NUMS_OF_HEAD, num_blks=NUMS_OF_BLKS, dropout=0.2)\n",
        "\n",
        "devices = d2l.try_all_gpus()\n",
        "loss = nn.CrossEntropyLoss()\n",
        "\n",
        "def _get_batch_loss_bert(net, loss, vocab_size, tokens_X,\n",
        "                         segments_X, valid_lens_x,\n",
        "                         pred_positions_X, mlm_weights_X,\n",
        "                         mlm_Y, nsp_y):\n",
        "    # Forward pass\n",
        "    _, mlm_Y_hat, nsp_Y_hat = net(tokens_X, segments_X,\n",
        "                                  valid_lens_x.reshape(-1),\n",
        "                                  pred_positions_X)\n",
        "    # Compute masked language model loss\n",
        "    mlm_l = loss(mlm_Y_hat.reshape(-1, vocab_size), mlm_Y.reshape(-1)) *\\\n",
        "    mlm_weights_X.reshape(-1, 1)\n",
        "    mlm_l = mlm_l.sum() / (mlm_weights_X.sum() + 1e-8)\n",
        "    # Compute next sentence prediction loss\n",
        "    nsp_l = loss(nsp_Y_hat, nsp_y)\n",
        "    l = mlm_l + nsp_l\n",
        "    return mlm_l, nsp_l, l\n",
        "\n",
        "def train_bert(train_iter, net, loss, vocab_size, devices, num_steps):\n",
        "    net(*next(iter(train_iter))[:4])\n",
        "    net = nn.DataParallel(net, device_ids=devices).to(devices[0])\n",
        "    trainer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
        "    step, timer = 0, d2l.Timer()\n",
        "    #animator = d2l.Animator(xlabel='step', ylabel='loss',\n",
        "    #                        xlim=[1, num_steps], legend=['mlm', 'nsp'])\n",
        "    # Sum of masked language modeling losses, sum of next sentence prediction\n",
        "    # losses, no. of sentence pairs, count\n",
        "    metric = d2l.Accumulator(4)\n",
        "    num_steps_reached = False\n",
        "    while step < num_steps and not num_steps_reached:\n",
        "        for tokens_X, segments_X, valid_lens_x, pred_positions_X,\\\n",
        "            mlm_weights_X, mlm_Y, nsp_y in train_iter:\n",
        "            tokens_X = tokens_X.to(devices[0])\n",
        "            segments_X = segments_X.to(devices[0])\n",
        "            valid_lens_x = valid_lens_x.to(devices[0])\n",
        "            pred_positions_X = pred_positions_X.to(devices[0])\n",
        "            mlm_weights_X = mlm_weights_X.to(devices[0])\n",
        "            mlm_Y, nsp_y = mlm_Y.to(devices[0]), nsp_y.to(devices[0])\n",
        "            trainer.zero_grad()\n",
        "            timer.start()\n",
        "            mlm_l, nsp_l, l = _get_batch_loss_bert(\n",
        "                net, loss, vocab_size, tokens_X, segments_X, valid_lens_x,\n",
        "                pred_positions_X, mlm_weights_X, mlm_Y, nsp_y)\n",
        "            l.backward()\n",
        "            trainer.step()\n",
        "            metric.add(mlm_l, nsp_l, tokens_X.shape[0], 1)\n",
        "            timer.stop()\n",
        "            #animator.add(step + 1,\n",
        "            #             (metric[0] / metric[3], metric[1] / metric[3]))\n",
        "            step += 1\n",
        "            if step%500 == 0:\n",
        "              print(f\"The {step} has complited ,\\n MLM loss {metric[0] / metric[3]:.3f}, \\n NSP loss {metric[1] / metric[3]:.3f}\")\n",
        "            if step == num_steps:\n",
        "                num_steps_reached = True\n",
        "                break\n",
        "\n",
        "    print(f'MLM loss {metric[0] / metric[3]:.3f}, '\n",
        "          f'NSP loss {metric[1] / metric[3]:.3f}')\n",
        "    print(f'{metric[2] / timer.sum():.1f} sentence pairs/sec on '\n",
        "          f'{str(devices)}')"
      ],
      "metadata": {
        "id": "rxy8COd8MoqD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42397df5-ac34-44c8-9062-bfc0b384efe7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_bert(train_iter, net, loss, len(vocab), devices, 5000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GTF0OTuN2uT",
        "outputId": "9b7790a3-467b-42c6-8d22-ac7d6c51ee17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 500 has complited ,\n",
            " MLM loss 3.731, \n",
            " NSP loss 0.735\n",
            "The 1000 has complited ,\n",
            " MLM loss 3.700, \n",
            " NSP loss 0.730\n",
            "The 1500 has complited ,\n",
            " MLM loss 3.680, \n",
            " NSP loss 0.722\n",
            "The 2000 has complited ,\n",
            " MLM loss 3.667, \n",
            " NSP loss 0.715\n",
            "The 2500 has complited ,\n",
            " MLM loss 3.663, \n",
            " NSP loss 0.711\n",
            "The 3000 has complited ,\n",
            " MLM loss 3.662, \n",
            " NSP loss 0.709\n",
            "The 3500 has complited ,\n",
            " MLM loss 3.659, \n",
            " NSP loss 0.707\n",
            "The 4000 has complited ,\n",
            " MLM loss 3.658, \n",
            " NSP loss 0.705\n",
            "The 4500 has complited ,\n",
            " MLM loss 3.659, \n",
            " NSP loss 0.704\n",
            "The 5000 has complited ,\n",
            " MLM loss 3.659, \n",
            " NSP loss 0.703\n",
            "MLM loss 3.659, NSP loss 0.703\n",
            "553.0 sentence pairs/sec on [device(type='cuda', index=0)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def get_tokens_and_segments(tokens_a, tokens_b=None):\n",
        "    \"\"\"Get tokens of the BERT input sequence and their segment IDs.\n",
        "\n",
        "    Defined in :numref:`sec_bert`\"\"\"\n",
        "    \n",
        "    tokens = ['<cls>'] + tokens_a + ['<sep>']\n",
        "    \n",
        "    # 0 and 1 are marking segment A and B, respectively\n",
        "    segments = [0] * (len(tokens_a) + 2)\n",
        "    if tokens_b is not None:\n",
        "        tokens += tokens_b + ['<sep>']\n",
        "        segments += [1] * (len(tokens_b) + 1)\n",
        "    return tokens, segments\n",
        "\n",
        "def get_bert_encoding(net, tokens_a, tokens_b=None):\n",
        "    tokens, segments = get_tokens_and_segments(tokens_a, tokens_b)\n",
        "    token_ids = torch.tensor(vocab[tokens], device=devices[0]).unsqueeze(0)\n",
        "    segments = torch.tensor(segments, device=devices[0]).unsqueeze(0)\n",
        "    valid_len = torch.tensor(len(tokens), device=devices[0]).unsqueeze(0)\n",
        "    encoded_X, _, _ = net(token_ids, segments, valid_len)\n",
        "    return encoded_X\n"
      ],
      "metadata": {
        "id": "v25l0XcXVcvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feedforward neural net"
      ],
      "metadata": {
        "id": "07ReQhbW5XF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class PaperDataset(Dataset):\n",
        "\n",
        "    def __init__(self, path, net):\n",
        "        f = open(path)\n",
        "        self.df = json.load(f)\n",
        "        f.close()\n",
        "        self.bert = net\n",
        "        self.MAXLEN = 1000\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        label = torch.zeros(100)\n",
        "        label[self.df[index]['proauthors']] = 1\n",
        "        abstract =list(self.df[index][\"abstract\"])\n",
        "        if len(abstract) >= self.MAXLEN - 2:\n",
        "\n",
        "          abstract = abstract[0:998]\n",
        "          abstract[-3] = 12\n",
        "        absrtract = d2l.get_tokens_and_segments(abstract)\n",
        "        encoded_text_cls = get_bert_encoding(self.bert,abstract)[:, 0, :]\n",
        "        return encoded_text_cls.squeeze(0), label\n",
        "\n",
        "\n",
        "class SentimentClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, inputsize):\n",
        "        super(SentimentClassifier, self).__init__()\n",
        "        self.cls_layer = nn.Linear(inputsize, 100)\n",
        "\n",
        "    def forward(self, seq):\n",
        "        logits = self.cls_layer(seq)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "FV0jiNLq5bnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train(train_status, model, optim, criterion, epoch_size, train_loader, valid_loader):\n",
        "    device = 0\n",
        "    for epoch in range(epoch_size):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        epoch_labels = torch.Tensor([])\n",
        "        epoch_preds = torch.Tensor([])\n",
        "        TOTAL = len(train_loader)\n",
        "        train_loop = tqdm(enumerate(train_loader), total=TOTAL)\n",
        "        train_loop.set_description(f\"Epoch [{epoch+1}/{epoch_size}]\")\n",
        "        \n",
        "        for batch, (inputs, labels) in train_loop:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            optim.zero_grad()\n",
        "            loss.backward()\n",
        "            optim.step() \n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_preds = torch.cat(((epoch_preds, (outputs.cpu() > 0.5).int())), 0)\n",
        "            epoch_labels = torch.cat((epoch_labels, labels.cpu()), 0)\n",
        "\n",
        "            train_loop.set_postfix_str(\n",
        "                'train_loss={:.5f}'.format(loss.item())\n",
        "            )\n",
        "\n",
        "            if batch == TOTAL-1:\n",
        "                epoch_loss /= len(train_loader.dataset)/train_loader.batch_size\n",
        "                train_f1 = f1_score(epoch_labels, epoch_preds, average='samples', zero_division=1)\n",
        "                valid_f1 = validate(model, valid_loader)\n",
        "                print(epoch_preds.shape)\n",
        "                train_loop.set_postfix_str(\n",
        "                    'train_loss={:.5f}, train_f1={:.5f}, valid_f1={:.5f}'.format(\n",
        "                        epoch_loss, train_f1, valid_f1\n",
        "                    )\n",
        "                )\n",
        "\n",
        "def validate(model, valid_loader):\n",
        "    model.eval()\n",
        "    valid_labels = torch.Tensor([])\n",
        "    valid_preds = torch.Tensor([])\n",
        "    device = 0\n",
        "    with torch.no_grad():\n",
        "        for batch, (inputs, labels) in enumerate(valid_loader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            valid_preds = torch.cat(((valid_preds, (outputs.cpu() > 0.5).int())), 0)\n",
        "            valid_labels = torch.cat((valid_labels, labels.cpu()), 0)\n",
        "\n",
        "    return f1_score(valid_labels, valid_preds, average='samples', zero_division=1)"
      ],
      "metadata": {
        "id": "TeAoESTkXJtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epoch_size = 10\n",
        "batch_size = 2\n",
        "lr = 2e-3\n",
        "device = 0 \n",
        "model = SentimentClassifier(128).to(device)\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "train_set = PaperDataset('drive/MyDrive/SML2/train.json',net)\n",
        "valid_set = PaperDataset('drive/MyDrive/SML2/valid.json',net)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "\n",
        "train_status = {'train_loss': []}\n",
        "train(train_status, model, optim, criterion, epoch_size, train_loader, valid_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9Ohl0UndCNf",
        "outputId": "6bc16750-b135-4cdd-9d31-e22b52d8d6a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [1/10]: 100%|██████████| 10317/10317 [09:54<00:00, 17.35it/s, train_loss=0.03671, train_f1=0.71077, valid_f1=0.71080]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([20634, 100])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [2/10]: 100%|██████████| 10317/10317 [09:42<00:00, 17.72it/s, train_loss=0.02311, train_f1=0.71077, valid_f1=0.71080]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([20634, 100])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [3/10]: 100%|██████████| 10317/10317 [09:44<00:00, 17.65it/s, train_loss=0.02308, train_f1=0.71077, valid_f1=0.71080]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([20634, 100])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [4/10]: 100%|██████████| 10317/10317 [09:39<00:00, 17.81it/s, train_loss=0.02310, train_f1=0.71077, valid_f1=0.71080]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([20634, 100])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [5/10]: 100%|██████████| 10317/10317 [09:38<00:00, 17.83it/s, train_loss=0.02308, train_f1=0.71077, valid_f1=0.71080]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([20634, 100])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [6/10]: 100%|██████████| 10317/10317 [09:37<00:00, 17.86it/s, train_loss=0.02308, train_f1=0.71077, valid_f1=0.71080]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([20634, 100])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [7/10]:   2%|▏         | 165/10317 [00:07<08:03, 21.00it/s, train_loss=0.00343]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('drive/MyDrive/SML2/test.json')\n",
        "dfs = json.load(f)\n",
        "for df in dfs:\n",
        "  df[\"proauthors\"] = []\n",
        "f.close()\n",
        "with open('drive/MyDrive/SML2/test.json', \"w\") as outfile:\n",
        "    json.dump(dfs, outfile)\n",
        "print(dfs[0])"
      ],
      "metadata": {
        "id": "_qWVfeC6ROOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(\"drive/MyDrive/SML2/train.json\")\n",
        "df = json.load(f)\n",
        "f.close()\n",
        "print(len(df))\n",
        "print(len(vocab))"
      ],
      "metadata": {
        "id": "b41kWLnyIgkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_predictions(model, loader):\n",
        "    model.eval()\n",
        "    predict = []\n",
        "    device = 0\n",
        "    with torch.no_grad():\n",
        "      for batch, (inputs, labels) in enumerate(loader):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        predict.append((outputs.cpu() > 0.5).int())\n",
        "        #valid_labels = torch.cat((valid_labels, labels.cpu()), 0)\n",
        "    return predict\n",
        "TESTDataset = PaperDataset('drive/MyDrive/SML2/test.json',net)\n",
        "test_result = get_predictions(model,TESTDataset)\n",
        "for g in range(len(test_result)):\n",
        "  print(g, end =\", \")\n",
        "  for i in range(100):\n",
        "    if test_result[g][i] != 0:\n",
        "      print(i,end = \" \")\n",
        "  print(\"\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "UhxUHbf-K3Uj",
        "outputId": "054d0cff-d9c4-4440-f734-753d36abe567"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-086d9937ae9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m#valid_labels = torch.cat((valid_labels, labels.cpu()), 0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mTESTDataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPaperDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'drive/MyDrive/SML2/test.json'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mtest_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTESTDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'PaperDataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TESTDataset = PaperDataset('drive/MyDrive/SML2/test.json',net)\n",
        "test_result = get_predictions(model,TESTDataset)\n",
        "\n",
        "import csv\n",
        "with open(\"test.csv\",'w',newline='') as f:\n",
        "    csvwritter = csv.writer(f,delimiter = ',')\n",
        "    csvwritter.writerow([\"ID\",\"Predict\"])\n",
        "    for g in range(len(test_result)):\n",
        "        result = []\n",
        "        for i in range(len(test_result[g])):\n",
        "            if test_result[g][i] == 1:\n",
        "                result.append(i)\n",
        "        if result == []:\n",
        "            csvwritter.writerow([g,-1])\n",
        "        else:\n",
        "            out = \" \"\n",
        "            result = [str(r) for r in result]\n",
        "            csvwritter.writerow([g, out.join(result)])\n",
        "        "
      ],
      "metadata": {
        "id": "r8fHos9Tc6LI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('drive/MyDrive/SML2/test.json')\n",
        "dfs = json.load(f)\n",
        "print(len(dfs))"
      ],
      "metadata": {
        "id": "08RkWX7Ya480"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}