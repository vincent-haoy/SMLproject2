{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "6g8VdHBne1jz",
    "outputId": "2b8f632a-6a0d-45c1-b201-7d84024916c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (1.3.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (1.21.5)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (4.63.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: sklearn in /opt/conda/lib/python3.7/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from sklearn) (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.21.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sklearn) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.7.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: gensim==4.2.0 in /opt/conda/lib/python3.7/site-packages (4.2.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.7/site-packages (from gensim==4.2.0) (1.7.3)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /opt/conda/lib/python3.7/site-packages (from gensim==4.2.0) (1.21.5)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.7/site-packages (from gensim==4.2.0) (6.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: Cython in /opt/conda/lib/python3.7/site-packages (0.29.32)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install tqdm\n",
    "!pip install sklearn\n",
    "!pip install gensim==4.2.0\n",
    "!pip install Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JbkGG-nlmzSI"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "nm169Kxtd-bV"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import gensim\n",
    "import Cython\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "6fZS7S0PvhHh",
    "outputId": "8bf1da1b-1f4f-4d30-88fe-10c928052cb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Oct 12 16:52:50 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.48.07    Driver Version: 515.48.07    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:02:00.0 Off |                  N/A |\n",
      "|  0%   48C    P8    20W / 350W |      3MiB / 12288MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = json.load(open(r'data/backup/train.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "data = []\n",
    "d2v = gensim.models.doc2vec.Doc2Vec.load(r'doc2vec_256.model')\n",
    "n2v = gensim.models.Word2Vec.load(r\"node2vec_1024.model\")\n",
    "\n",
    "for i, record in enumerate(raw):\n",
    "    if i%1000 == 0:\n",
    "        print(i)\n",
    "        \n",
    "    proauthors = [i for i in record['authors'] if i < 100]\n",
    "    label = np.zeros(101)\n",
    "    # label[proauthors] = 1.\n",
    "    if len(proauthors) > 0:\n",
    "        label[proauthors] = 1.\n",
    "    else: \n",
    "        label[-1] = 1.\n",
    "    labels.append(label)\n",
    "    \n",
    "    text = [str(i) for i in record['title']]\n",
    "    text.extend([str(i) for i in record['abstract']])\n",
    "    text_vec = d2v.infer_vector(text)\n",
    "    # title_vec = d2v.infer_vector([str(i) for i in record['title']])\n",
    "    # abstract_vec = d2v.infer_vector([str(i) for i in record['abstract']])\n",
    "    # text_vec = np.concatenate([title_vec, abstract_vec], axis=0)\n",
    "    \n",
    "    # coauthors = np.zeros(21146)\n",
    "    # coauthors[[i-100 for i in [i for i in record['authors'] if i >= 100]]] = 1.\n",
    "    coauthors = [i for i in record['authors'] if i >= 100]\n",
    "    coauthor_vec = []\n",
    "    if len(coauthors) > 0:\n",
    "        for co in coauthors:\n",
    "            coauthor_vec.append(n2v.wv[str(co)])\n",
    "        coauthor_vec = np.mean(np.array(coauthor_vec), axis=0)\n",
    "    else:\n",
    "        coauthor_vec = np.zeros(1024)\n",
    "    \n",
    "    \n",
    "    venue_vec = np.zeros(465)\n",
    "    venue_vec[[record['venue']] if record['venue'] != '' else []] = 1.\n",
    "    \n",
    "    data.append(np.concatenate([text_vec, coauthor_vec, venue_vec], axis=0))\n",
    "    \n",
    "labels = np.array(labels)\n",
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = np.where(labels[:, -1]==0)\n",
    "data_t = data[ids]\n",
    "labels_t = labels[ids]\n",
    "\n",
    "ids = np.where(labels[:, -1]==1)\n",
    "data_f = data[ids]\n",
    "labels_f = labels[ids]\n",
    "\n",
    "data_t, labels_t = resample(data_t, labels_t, replace=True, n_samples=int(2 * labels_f.shape[0]), random_state=51)\n",
    "\n",
    "data = np.concatenate([data_t, data_f])\n",
    "labels = np.concatenate([labels_t, labels_f])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = StandardScaler()\n",
    "# data = scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54999, 1745)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54999, 101)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_valid, labels_train, labels_valid = train_test_split(data, labels, test_size=0.2, random_state=51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = TensorDataset(torch.tensor(data_train, dtype=torch.float), torch.tensor(labels_train, dtype=torch.float))\n",
    "valid_set = TensorDataset(torch.tensor(data_valid, dtype=torch.float), torch.tensor(labels_valid, dtype=torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "UqCD9F1lIGGv"
   },
   "outputs": [],
   "source": [
    "class AuthorAttriClf(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AuthorAttriClf, self).__init__()\n",
    "        \n",
    "        self.clf_block = nn.Sequential(\n",
    "            # nn.Linear(21867, 2048),\n",
    "            # nn.Dropout(),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(2048, 1024),\n",
    "            # nn.Dropout(),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(1024, 1024),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(1024, 101),\n",
    "            nn.Linear(1745, 1024),\n",
    "            nn.Dropout(),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.Dropout(),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 101),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        probs = self.clf_block(input)\n",
    "\n",
    "        return probs    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "qTd7tZgJO-tz"
   },
   "outputs": [],
   "source": [
    "def train(train_status, model, optim, scheduler, criterion, epoch_size, train_loader, valid_loader=None):\n",
    "   \n",
    "    # es_loss = 0\n",
    "    # es_count = 0\n",
    "    # es_patience = 5\n",
    "\n",
    "    for epoch in range(epoch_size):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        epoch_labels = torch.Tensor([]).to(device)\n",
    "        epoch_preds = torch.Tensor([]).to(device)\n",
    "\n",
    "        train_loop = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "        train_loop.set_description(f\"Epoch [{epoch+1}/{epoch_size}]\")\n",
    "\n",
    "        for batch, (inputs, labels) in train_loop:\n",
    "            time.sleep(0.01)\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step() \n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_preds = torch.cat(((epoch_preds, (outputs > 0.5).int())), 0)\n",
    "            epoch_labels = torch.cat((epoch_labels, labels), 0)\n",
    "\n",
    "            train_loop.set_postfix_str(\n",
    "                'train_loss={:.5f}'.format(loss.item())\n",
    "            )\n",
    "\n",
    "            if batch == len(train_loader)-1 and valid_loader is not None:\n",
    "                epoch_loss /= len(train_loader.dataset)/train_loader.batch_size\n",
    "                train_f1 = f1_score(epoch_labels.detach().cpu(), epoch_preds.detach().cpu(), average='samples', zero_division=1)\n",
    "                valid_loss, valid_f1 = validate(model, criterion, valid_loader)\n",
    "                train_loop.set_postfix_str(\n",
    "                    'train_loss={:.5f}, train_f1={:.5f}, valid_loss={:.5f}, valid_f1={:.5f}'.format(\n",
    "                        epoch_loss, train_f1, valid_loss, valid_f1\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                scheduler.step()\n",
    "                \n",
    "                # Early Stopping\n",
    "                # if es_loss != 0 and es_loss < valid_loss:\n",
    "                #     if es_count >= es_patience:\n",
    "                #         print('Early Stop')\n",
    "                #         return\n",
    "                #     else:\n",
    "                #         es_count += 1\n",
    "                # else:\n",
    "                #     es_loss = valid_loss\n",
    "                #     es_count = 0\n",
    "                \n",
    "def validate(model, criterion, valid_loader):\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    valid_labels = torch.Tensor([])\n",
    "    valid_preds = torch.Tensor([])\n",
    "    with torch.no_grad():\n",
    "        for batch, (inputs, labels) in enumerate(valid_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            valid_loss += loss.item()\n",
    "            valid_preds = torch.cat(((valid_preds, (outputs.detach().cpu() > 0.5).int())), 0)\n",
    "            valid_labels = torch.cat((valid_labels, labels.detach().cpu()), 0)\n",
    "    \n",
    "    valid_loss /= len(valid_loader.dataset) / valid_loader.batch_size\n",
    "    valid_f1 = f1_score(valid_labels, valid_preds, average='samples', zero_division=1)\n",
    "    return valid_loss, valid_f1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/50]: 100%|██████████| 344/344 [00:07<00:00, 43.54it/s, train_loss=0.06193, train_f1=0.09184, valid_loss=0.02725, valid_f1=0.27097]\n",
      "Epoch [2/50]: 100%|██████████| 344/344 [00:07<00:00, 46.00it/s, train_loss=0.02061, train_f1=0.45833, valid_loss=0.01378, valid_f1=0.69089]\n",
      "Epoch [3/50]: 100%|██████████| 344/344 [00:07<00:00, 45.31it/s, train_loss=0.01378, train_f1=0.66985, valid_loss=0.01079, valid_f1=0.78074]\n",
      "Epoch [4/50]: 100%|██████████| 344/344 [00:07<00:00, 45.40it/s, train_loss=0.01147, train_f1=0.73703, valid_loss=0.01015, valid_f1=0.79228]\n",
      "Epoch [5/50]: 100%|██████████| 344/344 [00:07<00:00, 44.49it/s, train_loss=0.01014, train_f1=0.77429, valid_loss=0.00868, valid_f1=0.83942]\n",
      "Epoch [6/50]: 100%|██████████| 344/344 [00:07<00:00, 45.15it/s, train_loss=0.00923, train_f1=0.79757, valid_loss=0.00820, valid_f1=0.86133]\n",
      "Epoch [7/50]: 100%|██████████| 344/344 [00:07<00:00, 45.36it/s, train_loss=0.00855, train_f1=0.81781, valid_loss=0.00776, valid_f1=0.87821]\n",
      "Epoch [8/50]: 100%|██████████| 344/344 [00:07<00:00, 46.70it/s, train_loss=0.00796, train_f1=0.83319, valid_loss=0.00743, valid_f1=0.88236]\n",
      "Epoch [9/50]: 100%|██████████| 344/344 [00:07<00:00, 44.63it/s, train_loss=0.00752, train_f1=0.84531, valid_loss=0.00707, valid_f1=0.88507]\n",
      "Epoch [10/50]: 100%|██████████| 344/344 [00:07<00:00, 45.85it/s, train_loss=0.00707, train_f1=0.85583, valid_loss=0.00669, valid_f1=0.89332]\n",
      "Epoch [11/50]: 100%|██████████| 344/344 [00:07<00:00, 45.05it/s, train_loss=0.00670, train_f1=0.86445, valid_loss=0.00666, valid_f1=0.90253]\n",
      "Epoch [12/50]: 100%|██████████| 344/344 [00:07<00:00, 46.72it/s, train_loss=0.00639, train_f1=0.87139, valid_loss=0.00648, valid_f1=0.89972]\n",
      "Epoch [13/50]: 100%|██████████| 344/344 [00:06<00:00, 51.83it/s, train_loss=0.00608, train_f1=0.87739, valid_loss=0.00635, valid_f1=0.90380]\n",
      "Epoch [14/50]: 100%|██████████| 344/344 [00:07<00:00, 45.33it/s, train_loss=0.00576, train_f1=0.88378, valid_loss=0.00626, valid_f1=0.90794]\n",
      "Epoch [15/50]: 100%|██████████| 344/344 [00:07<00:00, 45.72it/s, train_loss=0.00555, train_f1=0.88909, valid_loss=0.00592, valid_f1=0.91355]\n",
      "Epoch [16/50]: 100%|██████████| 344/344 [00:07<00:00, 47.66it/s, train_loss=0.00528, train_f1=0.89406, valid_loss=0.00580, valid_f1=0.91679]\n",
      "Epoch [17/50]: 100%|██████████| 344/344 [00:07<00:00, 44.81it/s, train_loss=0.00506, train_f1=0.89869, valid_loss=0.00567, valid_f1=0.91564]\n",
      "Epoch [18/50]: 100%|██████████| 344/344 [00:07<00:00, 46.25it/s, train_loss=0.00483, train_f1=0.90302, valid_loss=0.00573, valid_f1=0.92018]\n",
      "Epoch [19/50]: 100%|██████████| 344/344 [00:07<00:00, 45.57it/s, train_loss=0.00464, train_f1=0.90618, valid_loss=0.00541, valid_f1=0.92145]\n",
      "Epoch [20/50]: 100%|██████████| 344/344 [00:07<00:00, 45.41it/s, train_loss=0.00445, train_f1=0.91084, valid_loss=0.00569, valid_f1=0.91988]\n",
      "Epoch [21/50]: 100%|██████████| 344/344 [00:07<00:00, 45.92it/s, train_loss=0.00421, train_f1=0.91376, valid_loss=0.00531, valid_f1=0.92779]\n",
      "Epoch [22/50]: 100%|██████████| 344/344 [00:07<00:00, 47.02it/s, train_loss=0.00407, train_f1=0.91728, valid_loss=0.00528, valid_f1=0.92876]\n",
      "Epoch [23/50]: 100%|██████████| 344/344 [00:07<00:00, 45.85it/s, train_loss=0.00388, train_f1=0.91920, valid_loss=0.00507, valid_f1=0.93255]\n",
      "Epoch [24/50]: 100%|██████████| 344/344 [00:07<00:00, 48.36it/s, train_loss=0.00376, train_f1=0.92127, valid_loss=0.00506, valid_f1=0.93324]\n",
      "Epoch [25/50]: 100%|██████████| 344/344 [00:07<00:00, 46.15it/s, train_loss=0.00358, train_f1=0.92544, valid_loss=0.00505, valid_f1=0.93224]\n",
      "Epoch [26/50]: 100%|██████████| 344/344 [00:07<00:00, 45.30it/s, train_loss=0.00350, train_f1=0.92660, valid_loss=0.00499, valid_f1=0.93358]\n",
      "Epoch [27/50]: 100%|██████████| 344/344 [00:07<00:00, 45.89it/s, train_loss=0.00335, train_f1=0.92910, valid_loss=0.00499, valid_f1=0.93656]\n",
      "Epoch [28/50]: 100%|██████████| 344/344 [00:07<00:00, 44.14it/s, train_loss=0.00323, train_f1=0.93308, valid_loss=0.00506, valid_f1=0.93780]\n",
      "Epoch [29/50]: 100%|██████████| 344/344 [00:07<00:00, 46.49it/s, train_loss=0.00308, train_f1=0.93589, valid_loss=0.00477, valid_f1=0.93996]\n",
      "Epoch [30/50]: 100%|██████████| 344/344 [00:07<00:00, 45.51it/s, train_loss=0.00300, train_f1=0.93506, valid_loss=0.00477, valid_f1=0.94317]\n",
      "Epoch [31/50]: 100%|██████████| 344/344 [00:07<00:00, 45.06it/s, train_loss=0.00288, train_f1=0.94008, valid_loss=0.00489, valid_f1=0.94333]\n",
      "Epoch [32/50]: 100%|██████████| 344/344 [00:07<00:00, 46.06it/s, train_loss=0.00279, train_f1=0.94267, valid_loss=0.00472, valid_f1=0.94448]\n",
      "Epoch [33/50]: 100%|██████████| 344/344 [00:07<00:00, 45.44it/s, train_loss=0.00269, train_f1=0.94273, valid_loss=0.00472, valid_f1=0.94804]\n",
      "Epoch [34/50]: 100%|██████████| 344/344 [00:07<00:00, 46.34it/s, train_loss=0.00264, train_f1=0.94397, valid_loss=0.00477, valid_f1=0.94577]\n",
      "Epoch [35/50]: 100%|██████████| 344/344 [00:07<00:00, 45.52it/s, train_loss=0.00252, train_f1=0.94655, valid_loss=0.00477, valid_f1=0.94888]\n",
      "Epoch [36/50]: 100%|██████████| 344/344 [00:07<00:00, 46.09it/s, train_loss=0.00251, train_f1=0.94704, valid_loss=0.00459, valid_f1=0.94916]\n",
      "Epoch [37/50]: 100%|██████████| 344/344 [00:07<00:00, 49.04it/s, train_loss=0.00239, train_f1=0.95054, valid_loss=0.00470, valid_f1=0.95018]\n",
      "Epoch [38/50]: 100%|██████████| 344/344 [00:06<00:00, 49.50it/s, train_loss=0.00228, train_f1=0.95196, valid_loss=0.00468, valid_f1=0.94967]\n",
      "Epoch [39/50]: 100%|██████████| 344/344 [00:07<00:00, 46.61it/s, train_loss=0.00216, train_f1=0.95451, valid_loss=0.00460, valid_f1=0.95328]\n",
      "Epoch [40/50]: 100%|██████████| 344/344 [00:07<00:00, 46.93it/s, train_loss=0.00214, train_f1=0.95438, valid_loss=0.00471, valid_f1=0.95185]\n",
      "Epoch [41/50]: 100%|██████████| 344/344 [00:07<00:00, 45.51it/s, train_loss=0.00209, train_f1=0.95577, valid_loss=0.00464, valid_f1=0.95305]\n",
      "Epoch [42/50]: 100%|██████████| 344/344 [00:07<00:00, 45.94it/s, train_loss=0.00209, train_f1=0.95602, valid_loss=0.00469, valid_f1=0.95560]\n",
      "Epoch [43/50]: 100%|██████████| 344/344 [00:07<00:00, 46.43it/s, train_loss=0.00196, train_f1=0.95865, valid_loss=0.00465, valid_f1=0.95684]\n",
      "Epoch [44/50]: 100%|██████████| 344/344 [00:07<00:00, 47.06it/s, train_loss=0.00196, train_f1=0.95942, valid_loss=0.00450, valid_f1=0.95808]\n",
      "Epoch [45/50]: 100%|██████████| 344/344 [00:07<00:00, 45.39it/s, train_loss=0.00187, train_f1=0.95981, valid_loss=0.00452, valid_f1=0.95893]\n",
      "Epoch [46/50]: 100%|██████████| 344/344 [00:07<00:00, 46.26it/s, train_loss=0.00183, train_f1=0.96130, valid_loss=0.00441, valid_f1=0.96123]\n",
      "Epoch [47/50]: 100%|██████████| 344/344 [00:07<00:00, 46.48it/s, train_loss=0.00180, train_f1=0.96275, valid_loss=0.00484, valid_f1=0.95867]\n",
      "Epoch [48/50]: 100%|██████████| 344/344 [00:07<00:00, 45.78it/s, train_loss=0.00167, train_f1=0.96449, valid_loss=0.00447, valid_f1=0.96147]\n",
      "Epoch [49/50]: 100%|██████████| 344/344 [00:07<00:00, 46.33it/s, train_loss=0.00165, train_f1=0.96480, valid_loss=0.00441, valid_f1=0.96188]\n",
      "Epoch [50/50]: 100%|██████████| 344/344 [00:07<00:00, 43.72it/s, train_loss=0.00168, train_f1=0.96571, valid_loss=0.00469, valid_f1=0.95873]\n"
     ]
    }
   ],
   "source": [
    "epoch_size = 50\n",
    "batch_size = 128\n",
    "lr = 1e-3\n",
    "\n",
    "model = AuthorAttriClf().to(device)\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optim, lr_lambda=lambda epoch: 0.98)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "train_status = {'train_loss': []}\n",
    "train(train_status, model, optim, scheduler, criterion, epoch_size, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_json(r'data/backup/test.json')\n",
    "\n",
    "pred_ids = []\n",
    "for i in df_test['identifier']:\n",
    "    pred_ids.append(i)\n",
    "pred_ids = np.array(pred_ids)\n",
    "\n",
    "data_test = []\n",
    "for _, row in df_test.iterrows():\n",
    "    text = [str(i) for i in row['title']]\n",
    "    text.extend([str(i) for i in row['abstract']])\n",
    "    text_vec = d2v.infer_vector(text)\n",
    "    # title_vec = d2v.infer_vector([str(i) for i in row['title']])\n",
    "    # abstract_vec = d2v.infer_vector([str(i) for i in row['abstract']])\n",
    "    # text_vec = np.concatenate([title_vec, abstract_vec], axis=0)\n",
    "    \n",
    "    # coauthors = np.zeros(21146)\n",
    "    # coauthors[[i-100 for i in [i for i in row['coauthors'] if i >= 100]]] = 1.\n",
    "    coauthors = row['coauthors']\n",
    "    coauthor_vec = []\n",
    "    if len(coauthors) > 0:\n",
    "        for co in coauthors:\n",
    "            coauthor_vec.append(n2v.wv[str(co)])\n",
    "        coauthor_vec = np.mean(np.array(coauthor_vec), axis=0)\n",
    "    else:\n",
    "        coauthor_vec = np.zeros(1024)\n",
    "    \n",
    "    venue_vec = np.zeros(465)\n",
    "    venue_vec[[row['venue']] if row['venue'] != '' else []] = 1.\n",
    "    \n",
    "    data_test.append(np.concatenate([text_vec, coauthor_vec, venue_vec], axis=0))\n",
    "    \n",
    "data_test = np.array(data_test)\n",
    "\n",
    "test_set = TensorDataset(torch.tensor(data_test, dtype=torch.float), torch.tensor(pred_ids, dtype=torch.float))\n",
    "test_loader = DataLoader(test_set, batch_size=128, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 1745)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "2x3VIck3495a"
   },
   "outputs": [],
   "source": [
    "def get_predictions(model, test_loader):\n",
    "    with torch.no_grad():\n",
    "        preds = []\n",
    "        identifiers = []\n",
    "\n",
    "        for batch, (inputs, ids) in enumerate(test_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = torch.sigmoid(model(inputs))\n",
    "\n",
    "            for i in range(outputs.shape[0]):\n",
    "                identifiers.append(int(ids[i]))\n",
    "                pred = torch.nonzero((outputs[i].cpu() > 0.6)[:-1].int())\n",
    "                if len(pred) > 0:\n",
    "                    preds.append(\" \".join([str(int(i)) for i in pred]))\n",
    "                else:\n",
    "                    preds.append(\"-1\")\n",
    "        df = pd.DataFrame({'ID': identifiers, 'Predict': preds})\n",
    "        df.to_csv(r'data/pred.csv', sep=',', index=False, encoding='utf-8')\n",
    "\n",
    "get_predictions(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
