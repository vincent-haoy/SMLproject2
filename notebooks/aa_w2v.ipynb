{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "6g8VdHBne1jz",
    "outputId": "2b8f632a-6a0d-45c1-b201-7d84024916c5"
   },
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install tqdm\n",
    "!pip install sklearn\n",
    "!pip install gensim==4.2.0\n",
    "!pip install Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JbkGG-nlmzSI"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "nm169Kxtd-bV"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "import gensim\n",
    "import Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "6fZS7S0PvhHh",
    "outputId": "8bf1da1b-1f4f-4d30-88fe-10c928052cb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Oct  7 13:43:02 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:81:00.0 Off |                  N/A |\n",
      "| 30%   30C    P8    20W / 350W |      3MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proauthors(row):\n",
    "    return [i for i in row['authors'] if i < 100]\n",
    "\n",
    "def get_coauthors(row):\n",
    "    return [i for i in row['authors'] if i >= 100]\n",
    "\n",
    "def has_proauthors(row):\n",
    "    if row['label'][-1] == 1:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = json.load(open(r'data/backup/train.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "data = []\n",
    "d2v = gensim.models.doc2vec.Doc2Vec.load(r'doc2vec.model')\n",
    "\n",
    "for i, record in enumerate(raw):\n",
    "    if i%1000 == 0:\n",
    "        print(i)\n",
    "        \n",
    "    proauthors = [i for i in record['authors'] if i < 100]\n",
    "    label = np.zeros(101)\n",
    "    if len(proauthors) > 0:\n",
    "        label[proauthors] = 1.\n",
    "    else: \n",
    "        label[-1] = 1.\n",
    "    labels.append(label)\n",
    "    \n",
    "    text = [str(i) for i in record['title']]\n",
    "    text.extend([str(i) for i in record['abstract']])\n",
    "    text = d2v.infer_vector(text)\n",
    "    \n",
    "    coauthors = np.zeros(21146)\n",
    "    coauthors[[i-100 for i in [i for i in record['authors'] if i >= 100]]] = 1.\n",
    "    \n",
    "    venue = np.zeros(466)\n",
    "    venue[[record['venue']] if record['venue'] != '' else [-1]] = 1.\n",
    "    \n",
    "    data.append(np.concatenate([text, coauthors, venue], axis=0))\n",
    "    \n",
    "labels = np.stack(labels, axis=0)\n",
    "data = np.stack(data, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = np.where(labels[:, -1]==0)\n",
    "data_t = data[ids]\n",
    "labels_t = labels[ids]\n",
    "\n",
    "ids = np.where(labels[:, -1]==1)\n",
    "data_f = data[ids]\n",
    "labels_f = labels[ids]\n",
    "\n",
    "data_t, labels_t = resample(data_t, labels_t, replace=True, n_samples=int(labels_f.shape[0]), random_state=51)\n",
    "\n",
    "data = np.concatenate((data_t, data_f))\n",
    "labels = np.concatenate((labels_t, labels_f))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36666, 21868)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36666, 101)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_valid, labels_train, labels_valid = train_test_split(data, labels, test_size=0.2, random_state=51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = TensorDataset(torch.tensor(data_train, dtype=torch.float), torch.tensor(labels_train, dtype=torch.float))\n",
    "valid_set = TensorDataset(torch.tensor(data_valid, dtype=torch.float), torch.tensor(labels_valid, dtype=torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "UqCD9F1lIGGv"
   },
   "outputs": [],
   "source": [
    "class AuthorAttriClf(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AuthorAttriClf, self).__init__()\n",
    "        \n",
    "        self.clf_block = nn.Sequential(\n",
    "            nn.Linear(21868, 2048),\n",
    "            nn.Dropout(),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.Dropout(),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 101),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        probs = self.clf_block(input)\n",
    "\n",
    "        return probs    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "qTd7tZgJO-tz"
   },
   "outputs": [],
   "source": [
    "def train(train_status, model, optim, scheduler, criterion, epoch_size, train_loader, valid_loader):\n",
    "   \n",
    "    for epoch in range(epoch_size):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        epoch_labels = torch.Tensor([])\n",
    "        epoch_preds = torch.Tensor([])\n",
    "\n",
    "        train_loop = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "        train_loop.set_description(f\"Epoch [{epoch+1}/{epoch_size}]\")\n",
    "\n",
    "        for batch, (inputs, labels) in train_loop:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step() \n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_preds = torch.cat(((epoch_preds, (outputs.detach().cpu() > 0.5).int())), 0)\n",
    "            epoch_labels = torch.cat((epoch_labels, labels.detach().cpu()), 0)\n",
    "\n",
    "            train_loop.set_postfix_str(\n",
    "                'train_loss={:.5f}'.format(loss.item())\n",
    "            )\n",
    "\n",
    "            if batch == len(train_loader)-1:\n",
    "                epoch_loss /= len(train_loader.dataset)/train_loader.batch_size\n",
    "                train_f1 = f1_score(epoch_labels, epoch_preds, average='samples', zero_division=1)\n",
    "                valid_f1 = validate(model, valid_loader)\n",
    "                train_loop.set_postfix_str(\n",
    "                    'train_loss={:.5f}, train_f1={:.5f}, valid_f1={:.5f}'.format(\n",
    "                        epoch_loss, train_f1, valid_f1\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                scheduler.step()\n",
    "\n",
    "def validate(model, valid_loader):\n",
    "    model.eval()\n",
    "    valid_labels = torch.Tensor([])\n",
    "    valid_preds = torch.Tensor([])\n",
    "    with torch.no_grad():\n",
    "        for batch, (inputs, labels) in enumerate(valid_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            valid_preds = torch.cat(((valid_preds, (outputs.detach().cpu() > 0.5).int())), 0)\n",
    "            valid_labels = torch.cat((valid_labels, labels.detach().cpu()), 0)\n",
    "\n",
    "    return f1_score(valid_labels, valid_preds, average='samples', zero_division=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/50]: 100%|██████████| 230/230 [00:04<00:00, 48.52it/s, train_loss=0.07181, train_f1=0.21448, valid_f1=0.39924]\n",
      "Epoch [2/50]: 100%|██████████| 230/230 [00:04<00:00, 48.98it/s, train_loss=0.03552, train_f1=0.43553, valid_f1=0.42623]\n",
      "Epoch [3/50]: 100%|██████████| 230/230 [00:04<00:00, 49.90it/s, train_loss=0.02931, train_f1=0.47161, valid_f1=0.47813]\n",
      "Epoch [4/50]: 100%|██████████| 230/230 [00:04<00:00, 48.95it/s, train_loss=0.01656, train_f1=0.61282, valid_f1=0.67394]\n",
      "Epoch [5/50]: 100%|██████████| 230/230 [00:04<00:00, 50.12it/s, train_loss=0.00916, train_f1=0.78193, valid_f1=0.79175]\n",
      "Epoch [6/50]: 100%|██████████| 230/230 [00:04<00:00, 50.01it/s, train_loss=0.00632, train_f1=0.86511, valid_f1=0.83514]\n",
      "Epoch [7/50]: 100%|██████████| 230/230 [00:04<00:00, 49.88it/s, train_loss=0.00479, train_f1=0.90061, valid_f1=0.86195]\n",
      "Epoch [8/50]: 100%|██████████| 230/230 [00:04<00:00, 49.85it/s, train_loss=0.00402, train_f1=0.92178, valid_f1=0.86346]\n",
      "Epoch [9/50]: 100%|██████████| 230/230 [00:04<00:00, 49.29it/s, train_loss=0.00351, train_f1=0.93286, valid_f1=0.87704]\n",
      "Epoch [10/50]: 100%|██████████| 230/230 [00:04<00:00, 50.03it/s, train_loss=0.00308, train_f1=0.94037, valid_f1=0.88176]\n",
      "Epoch [11/50]: 100%|██████████| 230/230 [00:04<00:00, 49.46it/s, train_loss=0.00277, train_f1=0.94674, valid_f1=0.88092]\n",
      "Epoch [12/50]: 100%|██████████| 230/230 [00:04<00:00, 50.37it/s, train_loss=0.00256, train_f1=0.95155, valid_f1=0.88038]\n",
      "Epoch [13/50]: 100%|██████████| 230/230 [00:04<00:00, 48.06it/s, train_loss=0.00240, train_f1=0.95471, valid_f1=0.88390]\n",
      "Epoch [14/50]: 100%|██████████| 230/230 [00:04<00:00, 49.96it/s, train_loss=0.00223, train_f1=0.95705, valid_f1=0.88855]\n",
      "Epoch [15/50]: 100%|██████████| 230/230 [00:04<00:00, 49.09it/s, train_loss=0.00216, train_f1=0.95897, valid_f1=0.88882]\n",
      "Epoch [16/50]: 100%|██████████| 230/230 [00:04<00:00, 49.93it/s, train_loss=0.00202, train_f1=0.96078, valid_f1=0.88876]\n",
      "Epoch [17/50]: 100%|██████████| 230/230 [00:04<00:00, 50.04it/s, train_loss=0.00195, train_f1=0.96335, valid_f1=0.88972]\n",
      "Epoch [18/50]: 100%|██████████| 230/230 [00:05<00:00, 45.06it/s, train_loss=0.00184, train_f1=0.96431, valid_f1=0.89326]\n",
      "Epoch [19/50]: 100%|██████████| 230/230 [00:05<00:00, 45.63it/s, train_loss=0.00180, train_f1=0.96438, valid_f1=0.89017]\n",
      "Epoch [20/50]: 100%|██████████| 230/230 [00:05<00:00, 44.52it/s, train_loss=0.00170, train_f1=0.96631, valid_f1=0.89009]\n",
      "Epoch [21/50]: 100%|██████████| 230/230 [00:04<00:00, 46.18it/s, train_loss=0.00169, train_f1=0.96621, valid_f1=0.89167]\n",
      "Epoch [22/50]: 100%|██████████| 230/230 [00:04<00:00, 50.25it/s, train_loss=0.00160, train_f1=0.96766, valid_f1=0.89165]\n",
      "Epoch [23/50]: 100%|██████████| 230/230 [00:04<00:00, 49.29it/s, train_loss=0.00155, train_f1=0.96858, valid_f1=0.89158]\n",
      "Epoch [24/50]:  55%|█████▍    | 126/230 [00:02<00:01, 59.37it/s, train_loss=0.00204]"
     ]
    }
   ],
   "source": [
    "epoch_size = 50\n",
    "batch_size = 128\n",
    "lr = 1e-3\n",
    "\n",
    "model = AuthorAttriClf().to(device)\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optim, lr_lambda=lambda epoch: 0.95)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "train_status = {'train_loss': []}\n",
    "train(train_status, model, optim, scheduler, criterion, epoch_size, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_json(r'data/backup/test.json')\n",
    "\n",
    "pred_ids = []\n",
    "for i in df_test['identifier']:\n",
    "    pred_ids.append(i)\n",
    "pred_ids = np.array(pred_ids)\n",
    "\n",
    "data_test = []\n",
    "for _, row in df_test.iterrows():\n",
    "    text = [str(i) for i in row['title']]\n",
    "    text.extend([str(i) for i in row['abstract']])\n",
    "    text =d2v.infer_vector(text)\n",
    "    \n",
    "    coauthors = np.zeros(21146)\n",
    "    coauthors[[i-100 for i in row['coauthors']]] = 1.\n",
    "    \n",
    "    venue = np.zeros(465)\n",
    "    venue[[row['venue']] if row['venue'] != '' else []] = 1.\n",
    "    \n",
    "    data_test.append(np.concatenate([text, coauthors, venue], axis=0))\n",
    "data_test = np.stack(data_test)\n",
    "\n",
    "test_set = TensorDataset(torch.tensor(data_test, dtype=torch.float), torch.tensor(pred_ids, dtype=torch.float))\n",
    "test_loader = DataLoader(test_set, batch_size=128, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2x3VIck3495a"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4516/596038893.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'data/pred.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mget_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test_loader' is not defined"
     ]
    }
   ],
   "source": [
    "def get_predictions(model, test_loader):\n",
    "    with torch.no_grad():\n",
    "        preds = []\n",
    "        identifiers = []\n",
    "\n",
    "        for batch, (inputs, ids) in enumerate(test_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = torch.sigmoid(model(inputs))\n",
    "\n",
    "            for i in range(outputs.shape[0]):\n",
    "                identifiers.append(int(ids[i]))\n",
    "                pred = torch.nonzero((outputs[i].cpu() > 0.5).int())\n",
    "                if len(pred[:-1]) > 0:\n",
    "                    preds.append(\" \".join([str(int(i)) for i in pred[:-1]]))\n",
    "                else:\n",
    "                    preds.append(\"-1\")\n",
    "\n",
    "        df = pd.DataFrame({'ID': identifiers, 'Predict': preds})\n",
    "        df.to_csv(r'data/pred.csv', sep=',', index=False, encoding='utf-8')\n",
    "\n",
    "get_predictions(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
