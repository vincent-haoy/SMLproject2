{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76d3908f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2455,\n",
       "  1858,\n",
       "  2335,\n",
       "  1543,\n",
       "  1800,\n",
       "  1860,\n",
       "  2000,\n",
       "  2867,\n",
       "  1546,\n",
       "  1874,\n",
       "  2059,\n",
       "  1525,\n",
       "  2590,\n",
       "  4196,\n",
       "  12,\n",
       "  2634,\n",
       "  1543,\n",
       "  1800,\n",
       "  1586,\n",
       "  2866,\n",
       "  3595,\n",
       "  1866,\n",
       "  1670,\n",
       "  2000,\n",
       "  3743,\n",
       "  1542,\n",
       "  1650,\n",
       "  1527,\n",
       "  33,\n",
       "  4407,\n",
       "  1543,\n",
       "  1535,\n",
       "  1962,\n",
       "  1961,\n",
       "  1543,\n",
       "  33,\n",
       "  1700,\n",
       "  1543,\n",
       "  1535,\n",
       "  1647,\n",
       "  1546,\n",
       "  1580,\n",
       "  4720,\n",
       "  12,\n",
       "  1731,\n",
       "  4231,\n",
       "  2601,\n",
       "  1553,\n",
       "  1704,\n",
       "  1605,\n",
       "  2456,\n",
       "  1543,\n",
       "  3281,\n",
       "  1594,\n",
       "  4407,\n",
       "  2168,\n",
       "  1542,\n",
       "  1586,\n",
       "  3781,\n",
       "  2471,\n",
       "  1525,\n",
       "  1859,\n",
       "  1669,\n",
       "  2512,\n",
       "  4572,\n",
       "  1546,\n",
       "  1609,\n",
       "  3781,\n",
       "  2471,\n",
       "  1525,\n",
       "  3393,\n",
       "  12,\n",
       "  37,\n",
       "  1712,\n",
       "  1586,\n",
       "  4196,\n",
       "  1650,\n",
       "  1527,\n",
       "  3281,\n",
       "  1594,\n",
       "  4407,\n",
       "  1800,\n",
       "  4708,\n",
       "  1904,\n",
       "  2059,\n",
       "  2411,\n",
       "  12],\n",
       " [40,\n",
       "  1542,\n",
       "  1691,\n",
       "  2449,\n",
       "  1535,\n",
       "  3616,\n",
       "  2206,\n",
       "  1904,\n",
       "  1642,\n",
       "  1543,\n",
       "  1535,\n",
       "  2007,\n",
       "  1669,\n",
       "  2869,\n",
       "  1543,\n",
       "  2174,\n",
       "  2638,\n",
       "  55,\n",
       "  2035,\n",
       "  4628,\n",
       "  6,\n",
       "  47,\n",
       "  11,\n",
       "  7,\n",
       "  1546,\n",
       "  1535,\n",
       "  47,\n",
       "  3522,\n",
       "  2223,\n",
       "  1656,\n",
       "  12,\n",
       "  1606,\n",
       "  57,\n",
       "  4624,\n",
       "  2199,\n",
       "  1540,\n",
       "  2206,\n",
       "  1977,\n",
       "  1947,\n",
       "  2886,\n",
       "  1549,\n",
       "  2152,\n",
       "  11,\n",
       "  2886,\n",
       "  1977,\n",
       "  1549,\n",
       "  2452,\n",
       "  33,\n",
       "  3723,\n",
       "  4172,\n",
       "  1546,\n",
       "  4082,\n",
       "  3999,\n",
       "  3980,\n",
       "  1546,\n",
       "  1624,\n",
       "  1529,\n",
       "  1719,\n",
       "  11,\n",
       "  1546,\n",
       "  1719,\n",
       "  3491,\n",
       "  2206,\n",
       "  2212,\n",
       "  1525,\n",
       "  1535,\n",
       "  1531,\n",
       "  46,\n",
       "  1542,\n",
       "  1558,\n",
       "  2175,\n",
       "  1745,\n",
       "  2243,\n",
       "  6,\n",
       "  7,\n",
       "  12,\n",
       "  1606,\n",
       "  1642,\n",
       "  1542,\n",
       "  2830,\n",
       "  1527,\n",
       "  1535,\n",
       "  47,\n",
       "  11,\n",
       "  3522,\n",
       "  40,\n",
       "  1986,\n",
       "  1653,\n",
       "  4762,\n",
       "  2991,\n",
       "  1549,\n",
       "  2525,\n",
       "  1535,\n",
       "  4345,\n",
       "  12,\n",
       "  19,\n",
       "  15,\n",
       "  4,\n",
       "  1525,\n",
       "  15,\n",
       "  11,\n",
       "  2420,\n",
       "  1527,\n",
       "  1535,\n",
       "  4795,\n",
       "  34,\n",
       "  2144,\n",
       "  36,\n",
       "  2124,\n",
       "  1777,\n",
       "  12],\n",
       " [40,\n",
       "  1542,\n",
       "  1691,\n",
       "  2449,\n",
       "  1535,\n",
       "  2610,\n",
       "  1543,\n",
       "  1535,\n",
       "  1984,\n",
       "  1962,\n",
       "  1682,\n",
       "  2235,\n",
       "  1807,\n",
       "  1543,\n",
       "  1606,\n",
       "  1596,\n",
       "  42,\n",
       "  41,\n",
       "  1606,\n",
       "  1665,\n",
       "  40,\n",
       "  12,\n",
       "  1731,\n",
       "  1807,\n",
       "  2294,\n",
       "  1857,\n",
       "  1554,\n",
       "  1859,\n",
       "  1543,\n",
       "  1535,\n",
       "  2460,\n",
       "  2395,\n",
       "  1536,\n",
       "  1907,\n",
       "  1525,\n",
       "  1535,\n",
       "  3776,\n",
       "  47,\n",
       "  2223,\n",
       "  1656,\n",
       "  1527,\n",
       "  1904,\n",
       "  2035,\n",
       "  2063,\n",
       "  1534,\n",
       "  2085,\n",
       "  1719,\n",
       "  2614,\n",
       "  12,\n",
       "  46,\n",
       "  1535,\n",
       "  1691,\n",
       "  1557,\n",
       "  2227,\n",
       "  1672,\n",
       "  1535,\n",
       "  1807,\n",
       "  2294,\n",
       "  1667,\n",
       "  48,\n",
       "  1986,\n",
       "  10,\n",
       "  2140,\n",
       "  1606,\n",
       "  1596,\n",
       "  42,\n",
       "  41,\n",
       "  2086,\n",
       "  3854,\n",
       "  1586,\n",
       "  3610,\n",
       "  3257,\n",
       "  4851,\n",
       "  10,\n",
       "  1549,\n",
       "  1557,\n",
       "  3620,\n",
       "  1755,\n",
       "  1553,\n",
       "  1904,\n",
       "  1606,\n",
       "  1665,\n",
       "  40,\n",
       "  1704,\n",
       "  1810,\n",
       "  3395,\n",
       "  1652,\n",
       "  1643,\n",
       "  1657,\n",
       "  1877,\n",
       "  2846,\n",
       "  12]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import Counter\n",
    "def dict_from_json(filename):\n",
    "    f = open(filename)\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "    return data\n",
    "train_dict = dict_from_json(\"train.json\")\n",
    "tests = [train_dict[i]['abstract'] for i in range(3)]\n",
    "\n",
    "\"\"\"for i in range(len(test)):\n",
    "    print(len(test[i]['abstract']))\"\"\"\n",
    "#tf.one_hot(test, depth=2455)\n",
    "tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "008c1298",
   "metadata": {},
   "outputs": [],
   "source": [
    "#12 142145 / 24853\n",
    "#15 : 6338 /117\n",
    "#11 79312 / 55\n",
    "#7  29421 / 54\n",
    "#302 44  / 43\n",
    "#2564 309 / 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94b4971d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5092af17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myWikiTextDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Defined in :numref:`subsec_prepare_mlm_data`\"\"\"\n",
    "    def __init__(self, paragraphs, max_len):\n",
    "        # Input `paragraphs[i]` is a list of sentence strings representing a\n",
    "        # paragraph; while output `paragraphs[i]` is a list of sentences\n",
    "        # representing a paragraph, where each sentence is a list of tokens\n",
    "        paragraphs = [paragraph for paragraph in paragraphs]\n",
    "        sentences = [sentence for paragraph in paragraphs\n",
    "                     for sentence in paragraph]\n",
    "        self.vocab = MYVocab(sentences, min_freq=5, reserved_tokens=[\n",
    "            '<pad>', '<mask>', '<cls>', '<sep>'])\n",
    "        # Get data for the next sentence prediction task\n",
    "        examples = []\n",
    "        for paragraph in paragraphs:\n",
    "            examples.extend(d2l._get_nsp_data_from_paragraph(\n",
    "                paragraph, paragraphs, self.vocab, max_len))\n",
    "        # Get data for the masked language model task\n",
    "        examples = [(d2l._get_mlm_data_from_tokens(tokens, self.vocab)\n",
    "                      + (segments, is_next))\n",
    "                     for tokens, segments, is_next in examples]\n",
    "        self.examples = examples\n",
    "        # Pad inputs\n",
    "        (self.all_token_ids, self.all_segments, self.valid_lens,\n",
    "         self.all_pred_positions, self.all_mlm_weights,\n",
    "         self.all_mlm_labels, self.nsp_labels) = d2l._pad_bert_inputs(\n",
    "            examples, max_len, self.vocab)\n",
    "         \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.all_token_ids[idx], self.all_segments[idx],\n",
    "                self.valid_lens[idx], self.all_pred_positions[idx],\n",
    "                self.all_mlm_weights[idx], self.all_mlm_labels[idx],\n",
    "                self.nsp_labels[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_token_ids)\n",
    "    \n",
    "from collections import Counter    \n",
    "class MYVocab:\n",
    "    \"\"\"Vocabulary for text.\"\"\"\n",
    "    def __init__(self, tokens=[], min_freq=0, reserved_tokens=[]):\n",
    "        \"\"\"Defined in :numref:`sec_text-sequence`\"\"\"\n",
    "        # Flatten a 2D list if needed\n",
    "        if tokens and isinstance(tokens[0], list):\n",
    "            tokens = [token for line in tokens for token in line]\n",
    "        # Count token frequencies\n",
    "        counter = Counter(tokens)\n",
    "        self.token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
    "                                  reverse=True)\n",
    "        # The list of unique tokens\n",
    "        comp1 = reserved_tokens\n",
    "        comp2 = [token for token, freq in self.token_freqs if freq >= min_freq]\n",
    "        self.idx_to_token = list(set(['<unk>'] + comp1 + comp2))\n",
    "        self.token_to_idx = {token: idx\n",
    "                             for idx, token in enumerate(self.idx_to_token)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        if hasattr(indices, '__len__') and len(indices) > 1:\n",
    "            return [self.idx_to_token[int(index)] for index in indices]\n",
    "        return self.idx_to_token[indices]\n",
    "\n",
    "    @property\n",
    "    def unk(self):  # Index for the unknown token\n",
    "        return self.token_to_idx['<unk>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e89f5e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def paragraph_builder(abstract):\n",
    "    size = len(abstract)\n",
    "    idx_list = [idx + 1 for idx, val in\n",
    "            enumerate(abstract) if val == 12]\n",
    "    \n",
    "    res = [abstract[i: j] for i, j in\n",
    "            zip([0] + idx_list, idx_list + \n",
    "            ([size] if idx_list[-1] != size else []))]\n",
    "    \n",
    "    if res[-1][-1] != 12:\n",
    "        res[-1].append(12)\n",
    "    return res\n",
    "\n",
    "def load_Mydataset(batch_size, max_len, asbtracts):\n",
    "    num_workers = 0\n",
    "    paragraphs = [paragraph_builder(paragraph) for paragraph in asbtracts]\n",
    "    train_set = myWikiTextDataset(paragraphs, max_len)\n",
    "    train_iter = torch.utils.data.DataLoader(train_set, batch_size,shuffle=True, num_workers=num_workers)\n",
    "    return train_iter, train_set.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f95eacc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2455, 1858, 2335, 1543, 1800, 1860, 2000, 2867, 1546, 1874, 2059, 1525, 2590, 4196, 12, 2634, 1543, 1800, 1586, 2866, 3595, 1866, 1670, 2000, 3743, 1542, 1650, 1527, 33, 4407, 1543, 1535, 1962, 1961, 1543, 33, 1700, 1543, 1535, 1647, 1546, 1580, 4720, 12, 1731, 4231, 2601, 1553, 1704, 1605, 2456, 1543, 3281, 1594, 4407, 2168, 1542, 1586, 3781, 2471, 1525, 1859, 1669, 2512, 4572, 1546, 1609, 3781, 2471, 1525, 3393, 12, 37, 1712, 1586, 4196, 1650, 1527, 3281, 1594, 4407, 1800, 4708, 1904, 2059, 2411, 12]\n",
      "[40, 1542, 1691, 2449, 1535, 3616, 2206, 1904, 1642, 1543, 1535, 2007, 1669, 2869, 1543, 2174, 2638, 55, 2035, 4628, 6, 47, 11, 7, 1546, 1535, 47, 3522, 2223, 1656, 12, 1606, 57, 4624, 2199, 1540, 2206, 1977, 1947, 2886, 1549, 2152, 11, 2886, 1977, 1549, 2452, 33, 3723, 4172, 1546, 4082, 3999, 3980, 1546, 1624, 1529, 1719, 11, 1546, 1719, 3491, 2206, 2212, 1525, 1535, 1531, 46, 1542, 1558, 2175, 1745, 2243, 6, 7, 12, 1606, 1642, 1542, 2830, 1527, 1535, 47, 11, 3522, 40, 1986, 1653, 4762, 2991, 1549, 2525, 1535, 4345, 12, 19, 15, 4, 1525, 15, 11, 2420, 1527, 1535, 4795, 34, 2144, 36, 2124, 1777, 12]\n",
      "[40, 1542, 1691, 2449, 1535, 2610, 1543, 1535, 1984, 1962, 1682, 2235, 1807, 1543, 1606, 1596, 42, 41, 1606, 1665, 40, 12, 1731, 1807, 2294, 1857, 1554, 1859, 1543, 1535, 2460, 2395, 1536, 1907, 1525, 1535, 3776, 47, 2223, 1656, 1527, 1904, 2035, 2063, 1534, 2085, 1719, 2614, 12, 46, 1535, 1691, 1557, 2227, 1672, 1535, 1807, 2294, 1667, 48, 1986, 10, 2140, 1606, 1596, 42, 41, 2086, 3854, 1586, 3610, 3257, 4851, 10, 1549, 1557, 3620, 1755, 1553, 1904, 1606, 1665, 40, 1704, 1810, 3395, 1652, 1643, 1657, 1877, 2846, 12]\n"
     ]
    }
   ],
   "source": [
    "for test in tests:\n",
    "    print(paragraph_builder(test))\n",
    "    print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "091e2ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = dict_from_json(\"train.json\")[0]['abstract']\n",
    "train_iter, vocab = load_Mydataset(32, 256, [test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f7e30fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = d2l.BERTModel(len(vocab), num_hiddens=128,\n",
    "                    ffn_num_hiddens=256, num_heads=2, num_blks=2, dropout=0.2)\n",
    "\n",
    "#devices = d2l.try_all_gpus()\n",
    "devices = [1]\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "def _get_batch_loss_bert(net, loss, vocab_size, tokens_X,\n",
    "                         segments_X, valid_lens_x,\n",
    "                         pred_positions_X, mlm_weights_X,\n",
    "                         mlm_Y, nsp_y):\n",
    "    # Forward pass\n",
    "    _, mlm_Y_hat, nsp_Y_hat = net(tokens_X, segments_X,\n",
    "                                  valid_lens_x.reshape(-1),\n",
    "                                  pred_positions_X)\n",
    "    # Compute masked language model loss\n",
    "    mlm_l = loss(mlm_Y_hat.reshape(-1, vocab_size), mlm_Y.reshape(-1)) *\\\n",
    "    mlm_weights_X.reshape(-1, 1)\n",
    "    mlm_l = mlm_l.sum() / (mlm_weights_X.sum() + 1e-8)\n",
    "    # Compute next sentence prediction loss\n",
    "    nsp_l = loss(nsp_Y_hat, nsp_y)\n",
    "    l = mlm_l + nsp_l\n",
    "    return mlm_l, nsp_l, l\n",
    "\n",
    "def train_bert(train_iter, net, loss, vocab_size, devices, num_steps):\n",
    "    net(*next(iter(train_iter))[:4])\n",
    "    net = nn.DataParallel(net, device_ids=devices).to(devices[0])\n",
    "    trainer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "    step, timer = 0, d2l.Timer()\n",
    "    animator = d2l.Animator(xlabel='step', ylabel='loss',\n",
    "                            xlim=[1, num_steps], legend=['mlm', 'nsp'])\n",
    "    # Sum of masked language modeling losses, sum of next sentence prediction\n",
    "    # losses, no. of sentence pairs, count\n",
    "    metric = d2l.Accumulator(4)\n",
    "    num_steps_reached = False\n",
    "    while step < num_steps and not num_steps_reached:\n",
    "        for tokens_X, segments_X, valid_lens_x, pred_positions_X,\\\n",
    "            mlm_weights_X, mlm_Y, nsp_y in train_iter:\n",
    "            tokens_X = tokens_X.to(devices[0])\n",
    "            segments_X = segments_X.to(devices[0])\n",
    "            valid_lens_x = valid_lens_x.to(devices[0])\n",
    "            pred_positions_X = pred_positions_X.to(devices[0])\n",
    "            mlm_weights_X = mlm_weights_X.to(devices[0])\n",
    "            mlm_Y, nsp_y = mlm_Y.to(devices[0]), nsp_y.to(devices[0])\n",
    "            trainer.zero_grad()\n",
    "            timer.start()\n",
    "            mlm_l, nsp_l, l = _get_batch_loss_bert(\n",
    "                net, loss, vocab_size, tokens_X, segments_X, valid_lens_x,\n",
    "                pred_positions_X, mlm_weights_X, mlm_Y, nsp_y)\n",
    "            l.backward()\n",
    "            trainer.step()\n",
    "            metric.add(mlm_l, nsp_l, tokens_X.shape[0], 1)\n",
    "            timer.stop()\n",
    "            animator.add(step + 1,\n",
    "                         (metric[0] / metric[3], metric[1] / metric[3]))\n",
    "            step += 1\n",
    "            if step == num_steps:\n",
    "                num_steps_reached = True\n",
    "                break\n",
    "\n",
    "    print(f'MLM loss {metric[0] / metric[3]:.3f}, '\n",
    "          f'NSP loss {metric[1] / metric[3]:.3f}')\n",
    "    print(f'{metric[2] / timer.sum():.1f} sentence pairs/sec on '\n",
    "          f'{str(devices)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bef3144f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_4684/3467621612.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_bert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_4684/1710722691.py\u001b[0m in \u001b[0;36mtrain_bert\u001b[1;34m(train_iter, net, loss, vocab_size, devices, num_steps)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrain_bert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataParallel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[0mtrainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md2l\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTimer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mto\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    925\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    926\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 927\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    928\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    929\u001b[0m     def register_backward_hook(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    600\u001b[0m             \u001b[1;31m# `with torch.no_grad():`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 602\u001b[1;33m                 \u001b[0mparam_applied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    603\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    604\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    923\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[0;32m    924\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[1;32m--> 925\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    926\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    927\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    209\u001b[0m                 \"multiprocessing, you must use the 'spawn' start method\")\n\u001b[0;32m    210\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_cuda_getDeviceCount'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m             raise AssertionError(\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "train_bert(train_iter, net, loss, len(vocab), devices, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "949a980f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.1+cpu\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a288e028",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
